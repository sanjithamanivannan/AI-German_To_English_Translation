{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**"
      ],
      "metadata": {
        "id": "8cUx6bx_Wz_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artificial Intelligence - Internship Project:**\n",
        "\n",
        "Language Translation (German to English)"
      ],
      "metadata": {
        "id": "bEdZsx4mXwPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "The objective of the project is to implement language\n",
        "translation model aka machine translation for converting\n",
        "German to English (and vice versa)"
      ],
      "metadata": {
        "id": "-e8YHcUqX4iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description:**\n",
        "\n",
        "For this project, the data is a text file (.txt) of English-German sentence\n",
        "pairs. The actual data contains over 150,000 sentence-pairs.\n",
        "However, only the first 50,000 sentence\n",
        "pairs will be used to reduce the training time of the model.\n"
      ],
      "metadata": {
        "id": "BVbKwvF2YBfV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGL0dNfae1dL"
      },
      "source": [
        "### 1.Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR1NuvlIHo4c"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth',200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v3Hw8RhfVRm"
      },
      "source": [
        "### 2.Function to read the data file\n",
        "The data is a text file of English-German sentence pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B18XiFwwL8vT"
      },
      "outputs": [],
      "source": [
        "#function to read raw text file\n",
        "def read_text(filename):\n",
        "  file = open(filename,mode='rt',encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5OjD8D0gbSO"
      },
      "source": [
        "### 3.Function to split the text\n",
        "The text data is split into English-German pairs separated by '\\n'.\n",
        "These pairs are then split into English and German sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfizTMA2Mr1B"
      },
      "outputs": [],
      "source": [
        "#split the text into sentences\n",
        "def to_lines(text):\n",
        "  sent = text.strip().split('\\n')\n",
        "  sent = [i.split('\\t') for i in sent]\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZfSPfL2iGKA"
      },
      "source": [
        "### 4.Reading & preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsNp49TcNFuq"
      },
      "outputs": [],
      "source": [
        "data = read_text(\"/content/Deu_dataset/deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgcSQJYQVFG4"
      },
      "outputs": [],
      "source": [
        "print(deu_eng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnCp5dasjTtN"
      },
      "source": [
        "The actual data contains over 150,000 sentence pairs. But only the first 50,000 sentence pairs will be used in order to reduce the training time of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lveErSqkN-jO"
      },
      "outputs": [],
      "source": [
        "deu_eng = deu_eng[:50000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK-_yGjhSFT0"
      },
      "outputs": [],
      "source": [
        "deu_eng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhuC9EPqlTPM"
      },
      "source": [
        "### 5.Text to Sequence Conversion\n",
        "The input and output sentences need to be converted into integer sequences of fixed length, so that it can be fed to Seq2Seq model. Two histograms are used to visualize the length of the sentences. The lengths of all the sentences are captured in two separate lists for English and German respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBiud2vzSyzj"
      },
      "outputs": [],
      "source": [
        "#empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "#populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "  eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "  deu_l.append(len(i.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCD1CvGIT6gc"
      },
      "outputs": [],
      "source": [
        "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZvcYkf2YHTa"
      },
      "outputs": [],
      "source": [
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the visualization, it can be seen that the maximum length of German sentences is 11 and that of English sentences is 8."
      ],
      "metadata": {
        "id": "YhF8ML7_DBBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.Function to vectorize the text data \n",
        "The text data is vectorized by using keras Tokenizer() class. This will turn the sentences into sequence of integers. Both the English and German sentences are vectorized."
      ],
      "metadata": {
        "id": "fVIf0XVbFM-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxqAV6-CYWLo"
      },
      "outputs": [],
      "source": [
        "#function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEvhgF87ZIqS"
      },
      "outputs": [],
      "source": [
        "#prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-LLmz4aaJgV"
      },
      "outputs": [],
      "source": [
        "#prepare deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:,1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.Function to pad the sequences\n",
        "The vectorized sequences will be padded with zeroes upto the maximum sequence length so that all the sequences are of the same length."
      ],
      "metadata": {
        "id": "HO0lEHu1G9cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rufLrFJMa8lR"
      },
      "outputs": [],
      "source": [
        "#encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(seq, maxlen = length, padding = 'post')\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkswoe0ooN2Y"
      },
      "source": [
        "### 6.Building the model\n",
        "The data is split into traininig and testing set for model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTcPTDNLoGKg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size = 0.2, random_state = 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.Encoding the sentences\n",
        "The German sentences are encoded as the input and English sentences as the target sequences."
      ],
      "metadata": {
        "id": "K9zaj19BM_4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z093J0H-omgL"
      },
      "outputs": [],
      "source": [
        "#prepare training data\n",
        "train_x = encode_sequences(deu_tokenizer, deu_length, train[:,1])\n",
        "train_y = encode_sequences(eng_tokenizer, eng_length, train[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEOymIWNpTFu"
      },
      "outputs": [],
      "source": [
        "#prepare validation data\n",
        "test_x = encode_sequences(deu_tokenizer, deu_length, test[:,1])\n",
        "test_y = encode_sequences(eng_tokenizer, eng_length, test[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.Seq2Seq Model\n",
        "An Embedding layer and an LSTM layer are used as the encoder and another LSTM layer followed by a Dense layer are used as the decoder."
      ],
      "metadata": {
        "id": "qxBT50ttN3OI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpt4Zy1SpsPT"
      },
      "outputs": [],
      "source": [
        "#build NMT model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(in_vocab, units, input_length= in_timesteps, mask_zero = True))\n",
        "  model.add(LSTM(units))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(LSTM(units, return_sequences= True))\n",
        "  model.add(Dense(out_vocab, activation = 'softmax'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop optimizer is used in this model as it is considered a good choice for Recurrent Neural Networks."
      ],
      "metadata": {
        "id": "wrNZExD8PyNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ7raR6Yq9zm"
      },
      "outputs": [],
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer = rms, loss = 'sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Training the model\n",
        "The model will be trained for 5 epochs and with a batch size of 512. The ModelCheckpoint() function is used to save the best model with lowest validation loss."
      ],
      "metadata": {
        "id": "1bHBh6R8QK9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYOJEYmirqQS"
      },
      "outputs": [],
      "source": [
        "filename = 'model1'\n",
        "checkpoint = ModelCheckpoint(filename, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min')\n",
        "history = model.fit(train_x, train_y.reshape(train_y.shape[0], train_y.shape[1], 1),\n",
        "                    epochs=5, batch_size =512, \n",
        "                    validation_split=0.2, \n",
        "                    callbacks = [checkpoint], verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of the training loss and validation loss using a visualization."
      ],
      "metadata": {
        "id": "Hy_bTkhwRjKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ITWrjTubPv"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Testing the model"
      ],
      "metadata": {
        "id": "pcBXlpYqSXT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBW7w8mxwSES"
      },
      "outputs": [],
      "source": [
        "model = load_model('model1')\n",
        "pred = model.predict(test_x.reshape((test_x.shape[0],test_x.shape[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEq9ky4FTYqh"
      },
      "outputs": [],
      "source": [
        "pred = argmax(pred,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZtdhRpEonxY"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKebdHA7xP82"
      },
      "outputs": [],
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == n:\n",
        "      return word\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79cHJYvvx8Fc"
      },
      "outputs": [],
      "source": [
        "#convert predictions into text in English\n",
        "pred_text = []\n",
        "for i in pred:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], eng_tokenizer)\n",
        "    if j>0:\n",
        "      if(t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "    else:\n",
        "      if(t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "pred_text.append(''.join(temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK9l6hH7G1dZ"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame({'actual':test[:,0],'predicted':pred_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4mWokrQxoO"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth',200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFex8-6LRL9D"
      },
      "outputs": [],
      "source": [
        "pred_df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFxx8UNdRXwy"
      },
      "outputs": [],
      "source": [
        "pred_df.tail(15)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI-Language_Translation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}